{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chainer_wrn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRcZFzh0OPKI",
        "colab_type": "text"
      },
      "source": [
        "# WideResNet in Chainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPcIzk08J3L9",
        "colab_type": "text"
      },
      "source": [
        "### Install Chainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU3pRtuBJ6p9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!curl https://colab.chainer.org/install | sh -"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhCu9vVEKLoo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import chainer as ch\n",
        "import numpy as np\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "from chainer import Chain, Sequential\n",
        "from chainer.training import extensions as E\n",
        "\n",
        "ch.config.autotune = True # Enable autotuner of cuDNN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkoifiS29wUk",
        "colab_type": "text"
      },
      "source": [
        "###Set the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmR2_39w92Lb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "DEPTH = 16\n",
        "WIDTH_FACTOR = 4\n",
        "WEIGHT_DECAY = 0.0005\n",
        "LABEL_SMOOTHING = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQeYm1hcKeZP",
        "colab_type": "text"
      },
      "source": [
        "### Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of7y9lMxKZsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicUnit(Chain):\n",
        "    def __init__(self, channels):\n",
        "        super(BasicUnit, self).__init__()\n",
        "        w = ch.initializers.HeNormal()\n",
        "        \n",
        "        with self.init_scope():\n",
        "            self.block = Sequential(\n",
        "                L.BatchNormalization(channels),\n",
        "                F.relu,\n",
        "                L.Convolution2D(in_channels=channels, out_channels=channels, ksize=3, stride=1, pad=1, nobias=True, initialW=w),\n",
        "                L.BatchNormalization(channels),\n",
        "                F.relu,\n",
        "                L.Convolution2D(in_channels=channels, out_channels=channels, ksize=3, stride=1, pad=1, nobias=True, initialW=w)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "\n",
        "class DownsampleUnit(Chain):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super(DownsampleUnit, self).__init__()\n",
        "        w = ch.initializers.HeNormal()\n",
        "        \n",
        "        with self.init_scope():\n",
        "            self.batchNorm = L.BatchNormalization(in_channels)\n",
        "            self.block = Sequential(\n",
        "                L.Convolution2D(in_channels=in_channels, out_channels=out_channels, ksize=3, stride=stride, pad=1, nobias=True, initialW=w),\n",
        "                L.BatchNormalization(out_channels),\n",
        "                F.relu,\n",
        "                L.Convolution2D(in_channels=out_channels, out_channels=out_channels, ksize=3, stride=1, pad=1, nobias=True, initialW=w)\n",
        "            )\n",
        "            self.downsample = L.Convolution2D(in_channels=in_channels, out_channels=out_channels, ksize=1, stride=stride, pad=0, nobias=True, initialW=w)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.batchNorm(F.relu(x))\n",
        "        return self.downsample(x) + self.block(x)\n",
        "\n",
        "\n",
        "class Block(Sequential):\n",
        "    def __init__(self, in_channels, out_channels, stride, depth):\n",
        "        super(Block, self).__init__(\n",
        "            DownsampleUnit(in_channels, out_channels, stride),\n",
        "            BasicUnit(out_channels).repeat(depth)\n",
        "        )\n",
        "    \n",
        "\n",
        "class WideResNet(Sequential):\n",
        "    def __init__(self, depth, width_factor, in_channels, labels):\n",
        "        self.filters = [16, 1 * 16 * width_factor, 2 * 16 * width_factor, 4 * 16 * width_factor]\n",
        "        self.block_depth = (depth - 4) // (3 * 2)\n",
        "        w = ch.initializers.HeNormal()\n",
        "\n",
        "        super(WideResNet, self).__init__(\n",
        "            L.Convolution2D(in_channels=in_channels, out_channels=self.filters[0], ksize=3, stride=1, pad=1, nobias=True, initialW=w),\n",
        "            Block(self.filters[0], self.filters[1], 1, self.block_depth),\n",
        "            Block(self.filters[1], self.filters[2], 2, self.block_depth),\n",
        "            Block(self.filters[2], self.filters[3], 2, self.block_depth),\n",
        "            L.BatchNormalization(self.filters[3]),\n",
        "            F.relu,\n",
        "            lambda x: F.average_pooling_2d(x, ksize=x.shape[2:]).reshape(-1, self.filters[3]),\n",
        "            L.Linear(in_size=self.filters[3], out_size=labels, initialW=w)\n",
        "        )\n",
        "        \n",
        "\n",
        "model = L.Classifier(WideResNet(DEPTH, WIDTH_FACTOR, in_channels=3, labels=10)).to_gpu(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOsqqEytST7H",
        "colab_type": "text"
      },
      "source": [
        "### Download CIFAR10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIdGpQvESZZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = ch.datasets.get_cifar10() # download\n",
        "valid, train = ch.datasets.split_dataset_random(train, 5000, seed=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50NKZk3cS4RI",
        "colab_type": "code",
        "outputId": "590bf550-7c8d-46ab-87de-63865c0537ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "print(f\"train size: {len(train)}, valid size: {len(valid)}, test size: {len(test)}\")\n",
        "\n",
        "im, label = train[0]\n",
        "images = np.array([im for im, _ in train])\n",
        "CIFAR_C, CIFAR_H, CIFAR_W = im.shape\n",
        "CIFAR_MEAN = images.mean((0,2,3))\n",
        "CIFAR_STD = images.std((0,2,3))\n",
        "\n",
        "print(f\"data shape: {CIFAR_C, CIFAR_H, CIFAR_W}\")\n",
        "print(f\"data mean: {CIFAR_MEAN}, std: {CIFAR_STD}, min: {images.min():.3f}, max: {images.max():.3f}\")     \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(np.transpose(im, axes=(1,2,0)))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train size: 45000, valid size: 5000, test size: 10000\n",
            "data shape: (3, 32, 32)\n",
            "data mean: [0.4914782  0.4822848  0.44654745], std: [0.24709967 0.24346308 0.26158434], min: 0.000, max: 1.000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niJyi4Bcip/x/bOCAsf02Fj8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EnDtY_4ZZ1Y",
        "colab_type": "text"
      },
      "source": [
        "### Define dataset augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd_AGUgfZY2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cutout(image, size=16, p=0.5):\n",
        "    if np.random.uniform() > p: return image\n",
        "    half_size = size // 2\n",
        "\n",
        "    left = np.random.randint(-half_size, CIFAR_W - half_size)\n",
        "    top = np.random.randint(-half_size, CIFAR_H - half_size)\n",
        "    right = min(CIFAR_W, left + size)\n",
        "    bottom = min(CIFAR_H, top + size)\n",
        "\n",
        "    image[:, max(0,left):right, max(0,top):bottom] = 0\n",
        "    return image\n",
        "\n",
        "def horizontal_flip(image, p=0.5):\n",
        "    if np.random.uniform() > p: return image\n",
        "    return np.fliplr(image)\n",
        "\n",
        "# does the same as padding by 4 and then randomly cropping\n",
        "def translate(image, amount=4):\n",
        "    clamp = lambda n: max(0, min(n, CIFAR_H))\n",
        "    top, left = np.random.randint(-amount, amount), np.random.randint(-amount, amount)\n",
        "\n",
        "    translated = np.zeros((CIFAR_C, CIFAR_H, CIFAR_W), dtype=np.float32)\n",
        "    translated[:, clamp(-top):clamp(-top + CIFAR_H), clamp(-left):clamp(-left + CIFAR_W)] = image[:, clamp(top):clamp(top + CIFAR_H), clamp(left):clamp(left + CIFAR_W)]\n",
        "\n",
        "    return translated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ePuQHNdZ_GO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def augment(input, train: bool):\n",
        "    img, label = input\n",
        "    img = img.copy()\n",
        "    \n",
        "    img -= CIFAR_MEAN[:, None, None]\n",
        "    img /= CIFAR_STD[:, None, None]\n",
        "\n",
        "    if train:\n",
        "        img = translate(horizontal_flip(cutout(img)))\n",
        "    \n",
        "    return img, label\n",
        "\n",
        "\n",
        "train = ch.datasets.TransformDataset(train, lambda im: augment(im, train=True))\n",
        "valid = ch.datasets.TransformDataset(valid, lambda im: augment(im, train=False))\n",
        "test = ch.datasets.TransformDataset(valid, lambda im: augment(im, train=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sUbEPNdjLyw",
        "colab_type": "text"
      },
      "source": [
        "### Define iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2n7Vk7xjN-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter = ch.iterators.MultiprocessIterator(train, BATCH_SIZE, repeat=True, shuffle=True)\n",
        "eval_iter = ch.iterators.MultiprocessIterator(valid, BATCH_SIZE, repeat=False, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxQGC75zlPTN",
        "colab_type": "text"
      },
      "source": [
        "### Define optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLbjoJOclRQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = ch.optimizers.NesterovAG(lr=0.1)\n",
        "optimizer.setup(model)\n",
        "optimizer.add_hook(ch.optimizer.WeightDecay(WEIGHT_DECAY))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11sRL6zonlbi",
        "colab_type": "text"
      },
      "source": [
        "### Define updater and trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUc9gXfOnnET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "updater = ch.training.StandardUpdater(train_iter, optimizer, device=0)\n",
        "trainer = ch.training.Trainer(updater, (200, 'epoch'), extensions=[\n",
        "    E.Evaluator(eval_iter, model, device=0),\n",
        "    E.observe_lr(),\n",
        "    E.LogReport(),\n",
        "    E.PlotReport(['main/loss', 'validation/main/loss'], x_key='epoch', file_name='loss.png'),\n",
        "    E.PlotReport(['main/accuracy', 'validation/main/accuracy'], x_key='epoch', file_name='accuracy.png'),\n",
        "    E.PrintReport(['epoch', 'main/loss', 'validation/main/loss', 'main/accuracy', 'validation/main/accuracy', 'elapsed_time', 'lr'])\n",
        "])\n",
        "trainer.extend(E.MultistepShift('lr', gamma=0.2, step_value=[60, 120, 160], init=0.1, optimizer=optimizer), trigger=(1, 'epoch'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDO5kQwKsVvY",
        "colab_type": "text"
      },
      "source": [
        "### Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPHsGXPjsVB4",
        "colab_type": "code",
        "outputId": "3e916982-01bf-4357-cc53-6c62744f6537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3541
        }
      },
      "source": [
        "trainer.run()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time  lr        \n",
            "\u001b[J1           1.49595     1.38524               0.448642       0.505859                  51.1318       0.1         \n",
            "\u001b[J2           1.12734     1.06142               0.592862       0.603125                  101.033       0.1         \n",
            "\u001b[J3           0.984469    0.945785              0.643697       0.661719                  150.309       0.1         \n",
            "\u001b[J4           0.892225    0.864989              0.682173       0.703125                  199.9         0.1         \n",
            "\u001b[J5           0.824127    0.837395              0.709647       0.704492                  249.272       0.1         \n",
            "\u001b[J6           0.778277    0.782662              0.727961       0.727539                  298.595       0.1         \n",
            "\u001b[J7           0.741633    0.780135              0.737558       0.727734                  347.755       0.1         \n",
            "\u001b[J8           0.712962    0.794082              0.749867       0.720508                  397.058       0.1         \n",
            "\u001b[J9           0.696934    0.745787              0.753795       0.750977                  446.364       0.1         \n",
            "\u001b[J10          0.670855    0.747284              0.768207       0.750977                  495.561       0.1         \n",
            "\u001b[J11          0.659882    0.698579              0.768311       0.761719                  544.837       0.1         \n",
            "\u001b[J12          0.644415    0.781691              0.773104       0.757422                  594.026       0.1         \n",
            "\u001b[J13          0.630923    0.666784              0.77832        0.774609                  643.363       0.1         \n",
            "\u001b[J14          0.616524    0.940042              0.784611       0.708398                  692.557       0.1         \n",
            "\u001b[J15          0.604597    0.679691              0.790283       0.766602                  741.891       0.1         \n",
            "\u001b[J16          0.592485    0.609785              0.79249        0.79375                   791.064       0.1         \n",
            "\u001b[J17          0.586702    0.540513              0.792658       0.817969                  840.357       0.1         \n",
            "\u001b[J18          0.576822    0.709291              0.799627       0.767187                  889.631       0.1         \n",
            "\u001b[J19          0.568967    0.643207              0.800214       0.783594                  938.761       0.1         \n",
            "\u001b[J20          0.565079    0.57103               0.801558       0.808594                  988.007       0.1         \n",
            "\u001b[J21          0.560703    0.581598              0.804999       0.804492                  1037.24       0.1         \n",
            "\u001b[J22          0.55255     0.684312              0.807817       0.768555                  1086.51       0.1         \n",
            "\u001b[J23          0.545378    0.557946              0.808249       0.808984                  1135.67       0.1         \n",
            "\u001b[J24          0.547382    0.70953               0.811213       0.771094                  1184.84       0.1         \n",
            "\u001b[J25          0.536945    0.563245              0.813432       0.810547                  1234          0.1         \n",
            "\u001b[J26          0.534819    0.65691               0.813568       0.792383                  1283          0.1         \n",
            "\u001b[J27          0.532243    0.661651              0.814631       0.776367                  1332.15       0.1         \n",
            "\u001b[J28          0.527044    0.614438              0.816284       0.799219                  1381.19       0.1         \n",
            "\u001b[J29          0.528253    0.625189              0.815674       0.792773                  1430.33       0.1         \n",
            "\u001b[J30          0.517566    0.536903              0.823451       0.820703                  1479.32       0.1         \n",
            "\u001b[J31          0.524286    0.589625              0.817716       0.804102                  1528.42       0.1         \n",
            "\u001b[J32          0.518467    0.532329              0.819979       0.818359                  1577.35       0.1         \n",
            "\u001b[J33          0.512928    0.541802              0.821711       0.816992                  1626.48       0.1         \n",
            "\u001b[J34          0.512818    0.696787              0.820956       0.760742                  1675.68       0.1         \n",
            "\u001b[J35          0.51259     0.696688              0.82236        0.766211                  1724.63       0.1         \n",
            "\u001b[J36          0.508525    0.660281              0.821045       0.781445                  1773.88       0.1         \n",
            "\u001b[J37          0.503411    0.617916              0.823384       0.791992                  1823.03       0.1         \n",
            "\u001b[J38          0.498101    0.782745              0.824951       0.748242                  1872.24       0.1         \n",
            "\u001b[J39          0.498275    0.653225              0.826634       0.78418                   1921.32       0.1         \n",
            "\u001b[J40          0.499682    0.527777              0.825306       0.828516                  1970.55       0.1         \n",
            "\u001b[J41          0.502425    0.657943              0.824774       0.77793                   2019.77       0.1         \n",
            "\u001b[J42          0.500271    0.643143              0.827435       0.801562                  2068.84       0.1         \n",
            "\u001b[J43          0.493217    0.662939              0.828835       0.790625                  2118.01       0.1         \n",
            "\u001b[J44          0.497863    0.706908              0.82739        0.778711                  2167.04       0.1         \n",
            "\u001b[J45          0.493335    0.540402              0.827504       0.821875                  2216.24       0.1         \n",
            "\u001b[J46          0.495614    0.558309              0.82926        0.810742                  2265.28       0.1         \n",
            "\u001b[J47          0.495652    0.584442              0.828769       0.810156                  2314.46       0.1         \n",
            "\u001b[J48          0.491952    0.6292                0.826678       0.795313                  2363.51       0.1         \n",
            "\u001b[J49          0.485789    0.649544              0.830922       0.80293                   2412.71       0.1         \n",
            "\u001b[J50          0.490484    0.590251              0.828258       0.805078                  2462.09       0.1         \n",
            "\u001b[J51          0.487747    0.806027              0.829549       0.7625                    2511.12       0.1         \n",
            "\u001b[J52          0.480285    0.667537              0.833385       0.778125                  2560.3        0.1         \n",
            "\u001b[J53          0.488604    0.733351              0.828414       0.765234                  2609.34       0.1         \n",
            "\u001b[J54          0.487821    0.708787              0.829656       0.772852                  2658.53       0.1         \n",
            "\u001b[J55          0.481893    0.50331               0.831953       0.836914                  2707.57       0.1         \n",
            "\u001b[J56          0.477451    0.514548              0.833097       0.827148                  2756.63       0.1         \n",
            "\u001b[J57          0.486253    0.567628              0.830677       0.812695                  2805.62       0.1         \n",
            "\u001b[J58          0.484346    0.608479              0.830507       0.796875                  2854.48       0.1         \n",
            "\u001b[J59          0.483644    0.611397              0.833363       0.801367                  2903.56       0.1         \n",
            "\u001b[J60          0.48736     0.632049              0.829193       0.79082                   2952.49       0.1         \n",
            "\u001b[J61          0.325225    0.297331              0.890159       0.904297                  3001.58       0.02        \n",
            "\u001b[J62          0.263527    0.283                 0.909299       0.901172                  3050.82       0.02        \n",
            "\u001b[J63          0.245114    0.299478              0.915394       0.902734                  3099.92       0.02        \n",
            "\u001b[J64          0.234524    0.291414              0.918002       0.904102                  3148.88       0.02        \n",
            "\u001b[J65          0.223113    0.281531              0.924583       0.907227                  3198.03       0.02        \n",
            "\u001b[J66          0.221279    0.286412              0.922763       0.905664                  3247.12       0.02        \n",
            "\u001b[J67          0.217308    0.303299              0.924034       0.903125                  3296.01       0.02        \n",
            "\u001b[J68          0.212916    0.323808              0.926003       0.899805                  3345.07       0.02        \n",
            "\u001b[J69          0.209798    0.305862              0.926772       0.902734                  3394          0.02        \n",
            "\u001b[J70          0.210501    0.286722              0.926558       0.906836                  3443.03       0.02        \n",
            "\u001b[J71          0.21257     0.32375               0.925614       0.897461                  3492.22       0.02        \n",
            "\u001b[J72          0.212276    0.340154              0.926824       0.891016                  3541.23       0.02        \n",
            "\u001b[J73          0.210398    0.350411              0.927069       0.885547                  3590.29       0.02        \n",
            "\u001b[J74          0.208013    0.307267              0.927907       0.900781                  3639.23       0.02        \n",
            "\u001b[J75          0.217112    0.340501              0.924294       0.89082                   3688.3        0.02        \n",
            "\u001b[J76          0.215785    0.319057              0.925592       0.896289                  3737.57       0.02        \n",
            "\u001b[J77          0.212182    0.330586              0.925626       0.890234                  3786.68       0.02        \n",
            "\u001b[J78          0.210668    0.299545              0.927484       0.900781                  3835.69       0.02        \n",
            "\u001b[J79          0.210508    0.321112              0.926802       0.900781                  3884.75       0.02        \n",
            "\u001b[J80          0.208625    0.343249              0.926816       0.894727                  3933.67       0.02        \n",
            "\u001b[J81          0.213512    0.310393              0.92587        0.901367                  3982.76       0.02        \n",
            "\u001b[J82          0.209191    0.326125              0.926713       0.89375                   4031.84       0.02        \n",
            "\u001b[J83          0.210233    0.306604              0.926015       0.904883                  4080.83       0.02        \n",
            "\u001b[J84          0.210613    0.366299              0.926558       0.888086                  4129.88       0.02        \n",
            "\u001b[J85          0.205876    0.295352              0.929532       0.904883                  4178.83       0.02        \n",
            "\u001b[J86          0.209977    0.318394              0.927357       0.902734                  4228          0.02        \n",
            "\u001b[J87          0.215914    0.303938              0.92508        0.899023                  4276.95       0.02        \n",
            "\u001b[J88          0.204322    0.340372              0.930664       0.887695                  4326.01       0.02        \n",
            "\u001b[J89          0.212751    0.316068              0.925582       0.900586                  4375.15       0.02        \n",
            "\u001b[J90          0.208172    0.337062              0.928241       0.893555                  4424.07       0.02        \n",
            "\u001b[J91          0.207404    0.295052              0.927601       0.905664                  4473.09       0.02        \n",
            "\u001b[J92          0.214103    0.371778              0.925147       0.885547                  4522.07       0.02        \n",
            "\u001b[J93          0.201173    0.309543              0.930087       0.898633                  4571.09       0.02        \n",
            "\u001b[J94          0.202635    0.355143              0.928975       0.888086                  4619.98       0.02        \n",
            "\u001b[J95          0.203245    0.328575              0.930109       0.899414                  4669.09       0.02        \n",
            "\u001b[J96          0.205294    0.40992               0.929532       0.876953                  4717.88       0.02        \n",
            "\u001b[J97          0.203224    0.326903              0.930398       0.889844                  4766.85       0.02        \n",
            "\u001b[J98          0.200024    0.351378              0.931152       0.885938                  4815.97       0.02        \n",
            "\u001b[J99          0.19739     0.337498              0.932269       0.891992                  4864.89       0.02        \n",
            "\u001b[J100         0.198785    0.437955              0.93153        0.866992                  4913.97       0.02        \n",
            "\u001b[J101         0.199125    0.357954              0.931735       0.888477                  4963.04       0.02        \n",
            "\u001b[J102         0.200041    0.317253              0.930176       0.896289                  5012.11       0.02        \n",
            "\u001b[J103         0.20087     0.338915              0.930845       0.892578                  5061.01       0.02        \n",
            "\u001b[J104         0.192124    0.339407              0.933172       0.892578                  5110.24       0.02        \n",
            "\u001b[J105         0.197407    0.350346              0.931774       0.886914                  5159.3        0.02        \n",
            "\u001b[J106         0.20181     0.33733               0.931067       0.894727                  5208.22       0.02        \n",
            "\u001b[J107         0.192393    0.330841              0.933305       0.899023                  5257.47       0.02        \n",
            "\u001b[J108         0.19464     0.312177              0.932626       0.901953                  5306.43       0.02        \n",
            "\u001b[J109         0.19342     0.345115              0.934126       0.893359                  5355.55       0.02        \n",
            "\u001b[J110         0.19642     0.380079              0.932025       0.87832                   5404.49       0.02        \n",
            "\u001b[J111         0.192582    0.341235              0.932906       0.895703                  5454.02       0.02        \n",
            "\u001b[J112         0.192466    0.340182              0.933494       0.895117                  5502.94       0.02        \n",
            "\u001b[J113         0.189207    0.379015              0.934504       0.888477                  5551.95       0.02        \n",
            "\u001b[J114         0.186356    0.31288               0.936035       0.900391                  5601.17       0.02        \n",
            "\u001b[J115         0.190744    0.314658              0.934762       0.90625                   5650.07       0.02        \n",
            "\u001b[J116         0.190442    0.34773               0.933461       0.897461                  5699.17       0.02        \n",
            "\u001b[J117         0.192667    0.361094              0.933805       0.883008                  5748.24       0.02        \n",
            "\u001b[J118         0.189539    0.334846              0.934304       0.888672                  5797.22       0.02        \n",
            "\u001b[J119         0.190521    0.369226              0.934295       0.887109                  5846.12       0.02        \n",
            "\u001b[J120         0.187483    0.302482              0.934126       0.904883                  5895.16       0.02        \n",
            "\u001b[J121         0.120587    0.221799              0.960627       0.928711                  5944.48       0.004       \n",
            "\u001b[J122         0.0906712   0.224755              0.971933       0.929297                  5993.45       0.004       \n",
            "\u001b[J123         0.0824001   0.225876              0.974432       0.928516                  6042.5        0.004       \n",
            "\u001b[J124         0.07692     0.2217                0.976229       0.929883                  6091.68       0.004       \n",
            "\u001b[J125         0.0697878   0.219274              0.977739       0.932813                  6140.76       0.004       \n",
            "\u001b[J126         0.0712975   0.211872              0.97703        0.93125                   6189.67       0.004       \n",
            "\u001b[J127         0.0699027   0.201328              0.978094       0.933398                  6238.76       0.004       \n",
            "\u001b[J128         0.0670744   0.216148              0.97792        0.933203                  6287.99       0.004       \n",
            "\u001b[J129         0.065121    0.213256              0.979048       0.933594                  6337.09       0.004       \n",
            "\u001b[J130         0.062984    0.216024              0.980402       0.933789                  6386.2        0.004       \n",
            "\u001b[J131         0.0626383   0.215233              0.979033       0.935547                  6435.19       0.004       \n",
            "\u001b[J132         0.0610538   0.223229              0.980558       0.932422                  6484.47       0.004       \n",
            "\u001b[J133         0.0587833   0.229316              0.982461       0.935742                  6533.45       0.004       \n",
            "\u001b[J134         0.0575903   0.234557              0.982488       0.929102                  6582.39       0.004       \n",
            "\u001b[J135         0.0563387   0.221974              0.982661       0.93418                   6631.39       0.004       \n",
            "\u001b[J136         0.0571235   0.227017              0.982045       0.931641                  6680.75       0.004       \n",
            "\u001b[J137         0.0555433   0.228216              0.982377       0.933789                  6729.73       0.004       \n",
            "\u001b[J138         0.0560655   0.2325                0.982483       0.929492                  6778.63       0.004       \n",
            "\u001b[J139         0.0580853   0.223603              0.981601       0.936133                  6827.62       0.004       \n",
            "\u001b[J140         0.0576479   0.225468              0.981771       0.929883                  6876.92       0.004       \n",
            "\u001b[J141         0.052951    0.219781              0.983066       0.932813                  6925.89       0.004       \n",
            "\u001b[J142         0.0543372   0.212561              0.982906       0.935156                  6974.8        0.004       \n",
            "\u001b[J143         0.0534157   0.22131               0.983953       0.934375                  7023.77       0.004       \n",
            "\u001b[J144         0.0553598   0.226795              0.982572       0.933594                  7073.13       0.004       \n",
            "\u001b[J145         0.0545073   0.220192              0.983132       0.931836                  7122.12       0.004       \n",
            "\u001b[J146         0.0552159   0.217761              0.982511       0.937305                  7171.17       0.004       \n",
            "\u001b[J147         0.0538839   0.215635              0.98373        0.934766                  7220.03       0.004       \n",
            "\u001b[J148         0.0535079   0.224392              0.982688       0.933398                  7269.52       0.004       \n",
            "\u001b[J149         0.0540921   0.215973              0.982951       0.937109                  7318.37       0.004       \n",
            "\u001b[J150         0.0538103   0.223565              0.982577       0.932813                  7367.4        0.004       \n",
            "\u001b[J151         0.0514395   0.229293              0.984063       0.93125                   7416.36       0.004       \n",
            "\u001b[J152         0.0522007   0.216862              0.98382        0.935156                  7465.74       0.004       \n",
            "\u001b[J153         0.0531445   0.229417              0.982733       0.931445                  7514.88       0.004       \n",
            "\u001b[J154         0.0535238   0.236903              0.98324        0.929492                  7563.68       0.004       \n",
            "\u001b[J155         0.0544748   0.217948              0.982799       0.935547                  7612.88       0.004       \n",
            "\u001b[J156         0.0513866   0.243866              0.983574       0.928906                  7661.66       0.004       \n",
            "\u001b[J157         0.0521789   0.222207              0.983754       0.93457                   7711.52       0.004       \n",
            "\u001b[J158         0.0509808   0.211519              0.983885       0.937109                  7760.28       0.004       \n",
            "\u001b[J159         0.0515188   0.231883              0.983576       0.931055                  7809.48       0.004       \n",
            "\u001b[J160         0.0537516   0.223893              0.982861       0.932227                  7858.27       0.004       \n",
            "\u001b[J161         0.0464727   0.207228              0.986239       0.938867                  7907.86       0.0008      \n",
            "\u001b[J162         0.0397302   0.203663              0.987882       0.941016                  7956.78       0.0008      \n",
            "\u001b[J163         0.0382004   0.20079               0.988137       0.941016                  8005.87       0.0008      \n",
            "\u001b[J164         0.0374887   0.201212              0.988969       0.941602                  8054.83       0.0008      \n",
            "\u001b[J165         0.0374024   0.199556              0.988804       0.939453                  8103.7        0.0008      \n",
            "\u001b[J166         0.035745    0.199018              0.988614       0.94043                   8153.4        0.0008      \n",
            "\u001b[J167         0.0364423   0.198358              0.989227       0.940625                  8202.31       0.0008      \n",
            "\u001b[J168         0.031821    0.198969              0.990168       0.940234                  8251.32       0.0008      \n",
            "\u001b[J169         0.0341517   0.198741              0.989502       0.938281                  8300.42       0.0008      \n",
            "\u001b[J170         0.0335392   0.200709              0.989695       0.937891                  8349.8        0.0008      \n",
            "\u001b[J171         0.0334998   0.199062              0.990234       0.938867                  8398.92       0.0008      \n",
            "\u001b[J172         0.0332731   0.199118              0.989517       0.938086                  8447.88       0.0008      \n",
            "\u001b[J173         0.032865    0.197486              0.989546       0.939062                  8496.96       0.0008      \n",
            "\u001b[J174         0.0326698   0.205483              0.990073       0.9375                    8545.85       0.0008      \n",
            "\u001b[J175         0.0321642   0.201675              0.990368       0.938672                  8595.29       0.0008      \n",
            "\u001b[J176         0.0308916   0.201814              0.990563       0.939844                  8644.12       0.0008      \n",
            "\u001b[J177         0.0304791   0.198697              0.990501       0.938672                  8693.09       0.0008      \n",
            "\u001b[J178         0.0301529   0.202352              0.9907         0.939062                  8742.21       0.0008      \n",
            "\u001b[J179         0.0318761   0.203324              0.989695       0.938281                  8791.14       0.0008      \n",
            "\u001b[J180         0.0302305   0.200839              0.991122       0.937109                  8840.73       0.0008      \n",
            "\u001b[J181         0.0303914   0.196765              0.991052       0.938867                  8889.73       0.0008      \n",
            "\u001b[J182         0.0311072   0.200284              0.990279       0.941016                  8938.83       0.0008      \n",
            "\u001b[J183         0.0297606   0.199672              0.991164       0.939258                  8987.78       0.0008      \n",
            "\u001b[J184         0.0303494   0.204074              0.990634       0.938477                  9036.74       0.0008      \n",
            "\u001b[J185         0.032282    0.198948              0.98948        0.938672                  9086.37       0.0008      \n",
            "\u001b[J186         0.0295622   0.199568              0.99152        0.939453                  9135.2        0.0008      \n",
            "\u001b[J187         0.0302272   0.19681               0.991189       0.939648                  9184.21       0.0008      \n",
            "\u001b[J188         0.0301167   0.19662               0.990986       0.940039                  9233.2        0.0008      \n",
            "\u001b[J189         0.0293792   0.200504              0.991211       0.939062                  9282.34       0.0008      \n",
            "\u001b[J190         0.0307667   0.192396              0.990852       0.942578                  9331.8        0.0008      \n",
            "\u001b[J191         0.0292489   0.198178              0.990945       0.938477                  9380.91       0.0008      \n",
            "\u001b[J192         0.0305719   0.204301              0.990674       0.939258                  9429.92       0.0008      \n",
            "\u001b[J193         0.0284683   0.203734              0.991344       0.940234                  9478.95       0.0008      \n",
            "\u001b[J194         0.0300349   0.203261              0.991056       0.938867                  9527.95       0.0008      \n",
            "\u001b[J195         0.0289768   0.205103              0.991408       0.939648                  9576.9        0.0008      \n",
            "\u001b[J196         0.0282137   0.200446              0.99181        0.939258                  9626.78       0.0008      \n",
            "\u001b[J197         0.0265589   0.201581              0.99241        0.940625                  9675.77       0.0008      \n",
            "\u001b[J198         0.0283349   0.198138              0.99181        0.940625                  9724.9        0.0008      \n",
            "\u001b[J199         0.0289127   0.199324              0.991631       0.939844                  9773.86       0.0008      \n",
            "\u001b[J200         0.0276678   0.199504              0.991877       0.941406                  9822.96       0.0008      \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}